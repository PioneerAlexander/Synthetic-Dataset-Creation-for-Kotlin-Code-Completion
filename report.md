# Synthetic Dataset Creation for Kotlin Code Completion
**JetBrains test task report (analysis part)**

There were two suggested models: codegen-350M-mono and granite-3b-code-base. 

The model [codegen-350M-mono](https://huggingface.co/Salesforce/codegen-350M-mono#codegen-codegen-mono-350m) was pretrained (after initialization with Multi-language model) with 71.7B tokens of Python programming language. It was hard to fine-tune such kind of model for writing on Kotlin thereafter. Especially because the synthetic dataset is around 1.4M tokens in total. Especially hard when the fine-tuning procedure is with LoRa... The model [granite-3b-code-base](https://huggingface.co/ibm-granite/granite-3b-code-base-2k) is purely multilingual, so we propose to fine-tune this model. The experiment results could potentially say more about the impact of a dataset.

The results of the granite model evaluation on KotlinHumanEval benchmark: [link](https://api.wandb.ai/links/kariakinaleksandr/jrq4rla7)


Dataset was created by translating the [code exercises](https://huggingface.co/datasets/jinaai/code_exercises) dataset to Kotlin using gpt-3.5-turbo model.


>  In addition, please discuss if we need to do filter the dataset you've obtained, and if yes -- how we should filter it.

Yes, the dataset I obtained should be filtered by several reasons:
    * The [code exercises](https://huggingface.co/datasets/jinaai/code_exercises) dataset has to be filtered. It was generated by ChatGPT-3, and some of the code samples are incorrect and/or incomplete. Consider, for example, even the very first item:

    ```python
    def calculate_average_price(prices): 
        """ Calculate the average price of a list of fashion items. Args: prices (list): A list of prices of fashion items. Returns: float: The average price of the fashion items. """	
        total = 0 
        
        while prices: 
            price = prices.pop(0) # Complete the missing code to update the total variable by adding the current price # Calculate the average price by dividing the total by the number of fashion items average_price = total / len(prices) return average_price

        average_price = total / len(prices) 
        
        return average_price
    ```
    It is clear that there us a bug: `total` is not updated.

    * The synthetic dataset was created by translating the dataset with bugs: the prompt we used was written as 'translate this code'. I had an idea that we can translate only a signature and docstring, and then ask for completing the code. However, this approach would require some more intelligent prompt-engineering. 

    * The model may give the wrong solution by itself, or give wrong translation. Sometimes, model refused to translate the text, sometimes, it wrote nothing.


    How the dataset should be filtered:
        * Ideally, as first step, check manually that there are no irrelevant, weird responses.  
        * Keep only the code which has no syntax errors (it affects evaluation on Kotlin HumanEval dataset btw).
        * More careful preprocess before passing to the model (ensure that syntax is correct)



