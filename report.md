# Synthetic Dataset Creation for Kotlin Code Completion
**JetBrains test task report (analysis part)**

There were two suggested models: codegen-350M-mono and granite-3b-code-base. 

The model [codegen-350M-mono](https://huggingface.co/Salesforce/codegen-350M-mono#codegen-codegen-mono-350m) was pretrained (after initialization with Multi-language model) with 71.7B tokens of Python programming language. It was hard to fine-tune such kind of model for writing on Kotlin thereafter. Especially because the synthetic dataset is around 1.4M tokens in total. Especially hard when the fine-tuning procedure is with LoRa... The model [granite-3b-code-base](https://huggingface.co/ibm-granite/granite-3b-code-base-2k) is purely multilingual, so we propose to fine-tune this model. The experiment results could potentially say more about the impact of a dataset.

Dataset was created by translating the [code exercises](https://huggingface.co/datasets/jinaai/code_exercises) dataset to Kotlin using gpt-3.5-turbo model.

The results of the granite model (pretrained) evaluation on KotlinHumanEval benchmark: [link](https://api.wandb.ai/links/kariakinaleksandr/jrq4rla7).

The model before finetuning scored 0.18. For comparison, Codegen-350-mono-350m scored 0.01. So we already observe an effect of "Textbook is all you need" paper. 

After finetuning process, we can ensure that the procedure went correctly by using WandB logs. 

Train loss graph: [link](https://api.wandb.ai/links/kariakinaleksandr/p67txpjk)
Eval loss graph: [link](https://wandb.ai/kariakinaleksandr/jb_synthetic_dataset_test_task/reports/eval-loss-24-11-04-23-32-31---VmlldzoxMDAyNTc1Nw)
Total run overview: [link](https://wandb.ai/kariakinaleksandr/jb_synthetic_dataset_test_task/runs/i226urqs/overview)

Then we run the evaluation of a finetuned model. The logs and the results can be found here: [link](https://wandb.ai/kariakinaleksandr/jb_synthetic_dataset_test_task/runs/vdxu0z5h/overview).



>  In addition, please discuss if we need to do filter the dataset you've obtained, and if yes -- how we should filter it.

Yes, the dataset I obtained should be filtered by several reasons:
    
    - The [code exercises](https://huggingface.co/datasets/jinaai/code_exercises) dataset has to be filtered. It was generated by ChatGPT-3, and some of the code samples are incorrect and/or incomplete. Consider, for example, even the very first item:

    ```python
    def calculate_average_price(prices): 
        """ Calculate the average price of a list of fashion items. Args: prices (list): A list of prices of fashion items. Returns: float: The average price of the fashion items. """	
        total = 0 
        
        while prices: 
            price = prices.pop(0) # Complete the missing code to update the total variable by adding the current price # Calculate the average price by dividing the total by the number of fashion items average_price = total / len(prices) return average_price

        average_price = total / len(prices) 
        
        return average_price
    ```
    It is clear that there us a bug: `total` is not updated in the loop.

    - The synthetic dataset was created by translating the dataset with bugs: the prompt we used was written as 'translate this code'. I had an idea that we can translate only a signature and docstring, and then ask for completing the code. However, this approach would require some more intelligent prompt-engineering. 

    - The model may give the wrong solution by itself, or give wrong translation. Sometimes, model refused to translate the text, sometimes, it wrote nothing.


    How the dataset should be filtered:
        - Ideally, as first step, check manually that there are no irrelevant, weird responses.  
        - Keep only the code which has no syntax errors (it affects evaluation on Kotlin HumanEval dataset btw).
        - More careful preprocess before passing to the model (ensure that syntax is correct)



